Session 7-8, Discriminant Analysis and Classification (Technical Slides)
========================================================
author : T. Evgeniou, INSEAD
title : Data Analytics for Business


```{r include=FALSE}
local_directory <- paste("/Users/theos/Documents/Theos/Teaching/MBA_COURSES/Data Analytics R version/CourseWebsite","CourseSessions/Sessions67", sep="/")
datafile_name="Boats" # do not add .csv at the end! make sure the data are numeric!!!! check your file!
cluster_file_ini = "Boats_cluster" # make sure this file exists in the "data" directory
dependent_variable= 82
independent_variables= c(54:80) # use 54-80 for boats
actual_1_predict_1 = 100
actual_1_predict_0 = -75
actual_0_predict_1 = -50
actual_0_predict_0 = 0
Probability_Threshold=50 # between 1 and 99%
estimation_data_percent = 80
validation_data_percent = 10
random_sampling = 0
CART_cp = 0.01
min_segment = 100
max_data_report = 50 # can also chance in server.R


Probability_Threshold = Probability_Threshold/100 # make it between 0 and 1
ProjectData <- read.csv(paste(paste(local_directory, "data", sep="/"), paste(datafile_name,"csv", sep="."), sep = "/"), sep=";", dec=",") # this contains only the matrix ProjectData
ProjectData=data.matrix(ProjectData)

if (datafile_name == "Boats")
  colnames(ProjectData)<-gsub("\\."," ",colnames(ProjectData))

dependent_variable = unique(sapply(dependent_variable,function(i) min(ncol(ProjectData), max(i,1))))
independent_variables = unique(sapply(independent_variables,function(i) min(ncol(ProjectData), max(i,1))))

if (length(unique(ProjectData[,dependent_variable])) !=2){
  new_dependent = ProjectData[,dependent_variable] >= median(ProjectData[,dependent_variable])
  ProjectData[,dependent_variable] <- 1*new_dependent
}

Profit_Matrix = matrix(c(actual_1_predict_1, actual_0_predict_1, actual_1_predict_0, actual_0_predict_0), ncol=2)
colnames(Profit_Matrix)<- c("Predict 1", "Predict 0")
rownames(Profit_Matrix) <- c("Actual 1", "Actual 0")
test_data_percent = 100-estimation_data_percent-validation_data_percent

source(paste(local_directory,"../../AnalyticsLibraries/library.R", sep="/"))
source(paste(local_directory,"../../AnalyticsLibraries/heatmapOutput.R", sep = "/"))

CART_control = rpart.control(cp = CART_cp)
```


Example Applications
========================================================

- Which molecules are most likely to succeed for the drug?
- Whose DNA is most likely to indicate future health problems of a particular type?
- Who are the most likely clients/companies/countries to default on their debt?
- Who are most likely to click on an ad? 
- To whom should we offer a particular promotion?
- How are satisfied customers different from dissatisfied customers in terms of their demographics and attitudes towards your productsâ€™ characteristics?
- Which transaction is most likely a fraud?
- Which applicants are most likely to fit in our organization and succeed?
- Which investments are most likely to succeed?



What is common to these problems?
========================================================

1. There is a dependent variable which is categorical e.g. success vs failure (fit vs. non-fit; fraud vs. non-fraud, response vs. non-response, etc.)

2. There are some independent variables which we can use to explain membership in the different categories



Example: Boats Purchase Drivers
========================================================

Who would be the most likely customers to purchase a boat in the future or to recommend their brand?

What would be the **key drivers** that affect people's decision to purchase or recommend?


Various Methods
========================================================

- Logistic regression
- Classification trees
- Boosted Trees
- Nearest Neighbors
- Neural Networks
- Bayesian methods
- Support Vector Machines
- Deep learning methods
- others...




Classification: A Process
========================================================

1. Create an estimation and two validation samples in a balanced way 
2. Setup the dependent variable (what is a success?)
3. Assess and select the independent variables
4. Estimate model (many methods, we consider only 2 here)
5. Assess performance on first validation data, repeat steps 2-5 as necessary
6. Assess performance on second validation data once



Data Splits: Example Split
========================================================

Estimation Data: `r estimation_data_percent`% of the data

Validation Data: `r validation_data_percent`% of the data

Test Data: `r 100 - estimation_data_percent - validation_data_percent`% of the data

```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE,results='asis'}

if (random_sampling){
  estimation_data_ids=sample.int(nrow(ProjectData),floor(estimation_data_percent*nrow(ProjectData)/100))
  non_estimation_data = setdiff(1:nrow(ProjectData),estimation_data_ids)
  validation_data_ids=non_estimation_data[sample.int(length(non_estimation_data), floor(validation_data_percent/(validation_data_percent+test_data_percent)*length(non_estimation_data)))]
  } else {
    estimation_data_ids=1:floor(estimation_data_percent*nrow(ProjectData)/100)
    non_estimation_data = setdiff(1:nrow(ProjectData),estimation_data_ids)
    validation_data_ids = (tail(estimation_data_ids,1)+1):(tail(estimation_data_ids,1) + floor(validation_data_percent/(validation_data_percent+test_data_percent)*length(non_estimation_data)))
    }

test_data_ids = setdiff(1:nrow(ProjectData), union(estimation_data_ids,validation_data_ids))

estimation_data=ProjectData[estimation_data_ids,]
validation_data=ProjectData[validation_data_ids,]
test_data=ProjectData[test_data_ids,]
```



Example: Some Data
========================================================
<style>
.wrapper{


width: 100%;

overflow-x: scroll;

}
.wrapper1{

height:450px;
overflow-y: scroll;
}
</style>
<div class="wrapper wrapper1">
```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE,results='asis'}
show_data = data.frame(round(ProjectData[,independent_variables],2))
show_data = show_data[1:min(max_data_report,nrow(show_data)),]
row<-rownames(show_data)
dfnew<-cbind(row,show_data)
change<-colnames(dfnew)
change[1]<-"Variables"
colnames (dfnew)<-change
#m1<-gvisTable(dfnew,options=list(showRowNumber=FALSE,width=960, height=440,allowHTML=TRUE,page='disable'))
#print(m1,'chart')
print(xtable(dfnew ,caption="Number of Observations per class in the Estimation Sample", digits=1,),type="html",html.table.attributes = "class='table table-striped table-hover table-bordered'",caption.placement="top",comment = FALSE,include.rownames = TRUE)
```
</div>



CART: Classification Trees
========================================================


```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}

independent_variables_nolabel = paste("IV", 1:length(independent_variables), sep="")

estimation_data_nolabel = cbind(estimation_data[,dependent_variable], estimation_data[,independent_variables])
colnames(estimation_data_nolabel)<- c(colnames(estimation_data)[dependent_variable],independent_variables_nolabel)

validation_data_nolabel = cbind(validation_data[,dependent_variable], validation_data[,independent_variables])
colnames(validation_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)

test_data_nolabel = cbind(test_data[,dependent_variable], test_data[,independent_variables])
colnames(test_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)

estimation_data_nolabel = data.frame(estimation_data_nolabel)
validation_data_nolabel = data.frame(validation_data_nolabel)
test_data_nolabel = data.frame(test_data_nolabel)

estimation_data = data.frame(estimation_data)
validation_data = data.frame(validation_data)
test_data = data.frame(test_data)

```
<center>
```{r echo=FALSE, message=FALSE, warning=FALSE,prompt=FALSE, results='asis'}
formula=paste(colnames(estimation_data)[dependent_variable],paste(Reduce(paste,sapply(head(independent_variables_nolabel,-1), function(i) paste(i,"+",sep=""))),tail(independent_variables_nolabel,1),sep=""),sep="~")
CART_tree<-rpart(formula, data= estimation_data_nolabel,method="class", control=CART_control)

rpart.plot(CART_tree, box.palette="OrBu", type=3, extra=1, fallen.leaves=F, branch.lty=3)
```
</center>



Another Classification Tree
========================================================

<center>
```{r echo=FALSE, warning=FALSE,message=FALSE, prompt=FALSE, results='asis'}
CART_tree_large<-rpart(formula, data= estimation_data_nolabel,method="class", control=rpart.control(cp = 0.005))

rpart.plot(CART_tree_large, box.palette="OrBu", type=3, extra=1, fallen.leaves=F, branch.lty=3)
```
</center>



KEY QUESTION: Model Complexity
========================================================

Do we want a "large" or a "small" tree? 

How complex should our classifier be?


```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
# Let's first calculate all probabilites for the estimation, validation, and test data
estimation_Probability_class1_tree<-predict(CART_tree, estimation_data_nolabel)[,2]
estimation_Probability_class1_tree_large<-predict(CART_tree_large, estimation_data_nolabel)[,2]

validation_Probability_class1_tree<-predict(CART_tree, validation_data_nolabel)[,2]
validation_Probability_class1_tree_large<-predict(CART_tree_large, validation_data_nolabel)[,2]

test_Probability_class1_tree<-predict(CART_tree, test_data_nolabel)[,2]
test_Probability_class1_tree_large<-predict(CART_tree_large, test_data_nolabel)[,2]


estimation_prediction_class_tree=1*as.vector(estimation_Probability_class1_tree > Probability_Threshold)
estimation_prediction_class_tree_large=1*as.vector(estimation_Probability_class1_tree_large > Probability_Threshold)

validation_prediction_class_tree=1*as.vector(validation_Probability_class1_tree > Probability_Threshold)
validation_prediction_class_tree_large=1*as.vector(validation_Probability_class1_tree_large > Probability_Threshold)

test_prediction_class_tree=1*as.vector(test_Probability_class1_tree > Probability_Threshold)
test_prediction_class_tree_large=1*as.vector(test_Probability_class1_tree_large > Probability_Threshold)

```

```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}

Classification_Table=rbind(validation_data[,dependent_variable],validation_Probability_class1_tree)
rownames(Classification_Table)<-c("Actual Class","Probability of Class 1")
colnames(Classification_Table)<- paste("Obs", 1:ncol(Classification_Table), sep=" ")

Classification_Table_large=rbind(validation_data[,dependent_variable],validation_Probability_class1_tree)
rownames(Classification_Table_large)<-c("Actual Class","Probability of Class 1")
colnames(Classification_Table_large)<- paste("Obs", 1:ncol(Classification_Table_large), sep=" ")

```



Logistic Regression
========================================================

<style>
.wrapper{
            
            
            width: 100%;
           
            overflow-x: scroll;
             
          }
.wrapper1{
            
           height:400px;
             overflow-y: scroll;
          }
</style>
<div class="wrapper wrapper1">
```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}

formula_log=paste(colnames(estimation_data[,dependent_variable,drop=F]),paste(Reduce(paste,sapply(head(independent_variables,-1), function(i) paste(colnames(estimation_data)[i],"+",sep=""))),colnames(estimation_data)[tail(independent_variables,1)],sep=""),sep="~")

logreg_solution <- glm(formula_log, family=binomial(link="logit"),  data=estimation_data)

log_coefficients = round(summary(logreg_solution)$coefficients,1)
print(xtable(log_coefficients,caption="Logistic Regression: Estimated Coefficients" , digits=1,),type="html",html.table.attributes = "class='table table-striped table-hover table-bordered'",caption.placement="top",comment = FALSE,include.rownames = TRUE)

```
</div>

```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
# Let's get the probabilities for the 3 types of data again
estimation_Probability_class1_log<-predict(logreg_solution, type="response", newdata=estimation_data[,independent_variables])
validation_Probability_class1_log<-predict(logreg_solution, type="response", newdata=validation_data[,independent_variables])
test_Probability_class1_log<-predict(logreg_solution, type="response", newdata=test_data[,independent_variables])

estimation_prediction_class_log=1*as.vector(estimation_Probability_class1_log > Probability_Threshold)
validation_prediction_class_log=1*as.vector(validation_Probability_class1_log > Probability_Threshold)
test_prediction_class_log=1*as.vector(test_Probability_class1_log > Probability_Threshold)

```



Drivers Analysis
========================================================

<style>
.wrapper{
            
            
            width: 100%;
           
            overflow-x: scroll;
             
          }
.wrapper1{
            
           height:400px;
             overflow-y: scroll;
          }
</style>
<div class="wrapper wrapper1">
```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE, results='asis'}
log_importance = tail(log_coefficients[,"z value", drop=F],-1) # remove the intercept
log_importance = log_importance/max(abs(log_importance))

tree_importance = CART_tree$variable.importance
tree_ordered_drivers = as.numeric(gsub("\\IV"," ",names(CART_tree$variable.importance)))
tree_importance_final = rep(0,length(independent_variables))
tree_importance_final[tree_ordered_drivers] <- tree_importance
tree_importance_final <- tree_importance_final/max(abs(tree_importance_final))
tree_importance_final <- tree_importance_final*sign(log_importance)

large_tree_importance = CART_tree_large$variable.importance
large_tree_ordered_drivers = as.numeric(gsub("\\IV"," ",names(CART_tree_large$variable.importance)))
large_tree_importance_final = rep(0,length(independent_variables))
large_tree_importance_final[large_tree_ordered_drivers] <- large_tree_importance
large_tree_importance_final <- large_tree_importance_final/max(abs(large_tree_importance_final))
large_tree_importance_final <- large_tree_importance_final*sign(log_importance)

Importance_table <- cbind(tree_importance_final,large_tree_importance_final, log_importance)
colnames(Importance_table) <- c("CART 1", "CART 2", "Logistic Regr.")
rownames(Importance_table) <- rownames(log_importance)
#printing the result in a clean-slate table
#cat(renderHeatmapX(Importance_table, border=1, center = 0, minvalue = 0))

print(xtable(Importance_table,caption="Logistic Regression: Estimated Coefficients" , digits=1,),type="html",html.table.attributes = "class='table table-striped table-hover table-bordered'",caption.placement="top",comment = FALSE,include.rownames = TRUE)


```
</div>



Hit Ratio: Validation Data
========================================================
<div class="row">
<div class="col-md-6">
```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
validation_actual=validation_data[,dependent_variable]
validation_predictions = rbind(validation_prediction_class_tree,
                               validation_prediction_class_tree_large,
                               validation_prediction_class_log)
validation_hit_rates = rbind(
  100*sum(validation_prediction_class_tree==validation_actual)/length(validation_actual), 
  100*sum(validation_prediction_class_tree_large==validation_actual)/length(validation_actual), 
  100*sum(validation_prediction_class_log==validation_actual)/length(validation_actual)
  )
colnames(validation_hit_rates) <- "Hit Ratio"
rownames(validation_hit_rates) <- c("First CART", "Second CART", "Logistic Regression")

print(xtable(validation_hit_rates ,caption="Validation Data Hit Ratios for different classifiers tested", digits=1,),type="html",html.table.attributes = "class='table table-striped table-hover table-bordered'",caption.placement="top",comment = FALSE,include.rownames = TRUE)

```
</div>
</div>



Hit Ratio: Estimation Data
========================================================
<div class="row">
<div class="col-md-6">
```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
estimation_actual=estimation_data[,dependent_variable]
estimation_predictions = rbind(estimation_prediction_class_tree,
                               estimation_prediction_class_tree_large,
                               estimation_prediction_class_log)
estimation_hit_rates = rbind(
  100*sum(estimation_prediction_class_tree==estimation_actual)/length(estimation_actual), 
  100*sum(estimation_prediction_class_tree_large==estimation_actual)/length(estimation_actual), 
  100*sum(estimation_prediction_class_log==estimation_actual)/length(estimation_actual)
  )
colnames(estimation_hit_rates) <- "Hit Ratio"
rownames(estimation_hit_rates) <- c("First CART", "Second CART", "Logistic Regression")

print(xtable(estimation_hit_rates ,caption="Estimation Data Hit Ratios for different classifiers tested", digits=1,),type="html",html.table.attributes = "class='table table-striped table-hover table-bordered'",caption.placement="top",comment = FALSE,include.rownames = TRUE)
```
</div>
</div>



Fit versus Prediction

========================================================
Should the performance of our model be similar in the estimation and validation data? 

How about when we deploy the model?

Why should performance be different? Why should it not? What can we do about it?




Hit Ratios: Test Data for best validation hit rate method
========================================================
<div class="row">
<div class="col-md-6">
```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
test_actual=test_data[,dependent_variable]
test_predictions = rbind(test_prediction_class_tree,
                         test_prediction_class_tree_large,
                         test_prediction_class_log)
test_hit_rates = rbind(
  100*sum(test_prediction_class_tree==test_actual)/length(test_actual), 
  100*sum(test_prediction_class_tree_large==test_actual)/length(test_actual), 
  100*sum(test_prediction_class_log==test_actual)/length(test_actual)
  )
colnames(test_hit_rates) <- "Hit Ratio"
rownames(test_hit_rates) <- c("First CART", "Second CART", "Logistic Regression")

print(xtable(test_hit_rates ,caption="Test Data Hit Ratios for different classifiers tested", digits=1,),type="html",html.table.attributes = "class='table table-striped table-hover table-bordered'",caption.placement="top",comment = FALSE,include.rownames = TRUE)

```
</div>
</div>



Confusion Matrix: Test Data
========================================================
<div class="row">
<div class="col-md-6">
```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
test_prediction_best = test_predictions[which.max(validation_hit_rates),]
conf_matrix = matrix(rep(0,4),ncol=2)
conf_matrix[1,1]<- 100*sum(test_prediction_best*test_data[,dependent_variable])/sum(test_data[,dependent_variable])
conf_matrix[1,2]<- 100*sum((!test_prediction_best)*test_data[,dependent_variable])/sum(test_data[,dependent_variable])
conf_matrix[2,1]<- 100*sum((!test_prediction_best)*(!test_data[,dependent_variable]))/sum((!test_data[,dependent_variable]))
conf_matrix[2,2]<- 100*sum((test_prediction_best)*(!test_data[,dependent_variable]))/sum((!test_data[,dependent_variable]))
conf_matrix = round(conf_matrix,2)

colnames(conf_matrix) <- c("Predicted 1", "Predicted 0")
rownames(conf_matrix) <- c("Actual 1", "Actual 0")

print(xtable(conf_matrix ,caption="Confusion Matrix for test data", digits=1,),type="html",html.table.attributes = "class='table table-striped table-hover table-bordered'",caption.placement="top",comment = FALSE)
```
</div>
</div>



ROC Curves: Test Data
========================================================

(black: CART 1; red: CART 2; blue: logistic regression):


```{r echo=FALSE,results='hide',include=FALSE,warning=FALSE,error=FALSE}

test_actual_class <- as.numeric(test_data[,dependent_variable])

pred_tree_test <- prediction(test_Probability_class1_tree, test_actual_class)
pred_tree_large_test <- prediction(test_Probability_class1_tree_large, test_actual_class)
pred_log_test <- prediction(test_Probability_class1_log, test_actual_class)
```


```{r echo=FALSE}
plot(performance(pred_tree_large_test, "tpr", "fpr"), col="red", lty=1, add=FALSE)
#grid()
par(new=TRUE)
plot(performance(pred_log_test, "tpr", "fpr"), col="blue", lty=1, add=FALSE)
par(new=TRUE)
plot(performance(pred_tree_test, "tpr", "fpr"),  lty=1, add=FALSE, main="ROC Curve")
par(new=FALSE)

```



Lift Curves: Test Data
========================================================


<style>
.wrapper{
            
            
            width: 100%;
           
            overflow-x: scroll;
             
          }
.wrapper1{
            
           height:450px;
             overflow-y: scroll;
          }
</style>
<div class="wrapper wrapper1">

```{r liftTest,echo=FALSE,results='asis',warning=FALSE,error=FALSE}
test_actual<- test_data[,dependent_variable]
all1s = sum(test_actual)

probs = test_Probability_class1_tree
xaxis = sort(unique(c(0,1,probs)), decreasing = TRUE)
res = 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly = which(probs >= 1-prob)
  c(length(useonly)/length(test_actual), sum(test_actual[useonly])/all1s) 
  }))
xaxis = res[1,]; yaxis = res[2,]
names(xaxis)<- NULL; names(yaxis) <- NULL
frame<-cbind(xaxis,yaxis)
frame<-as.data.frame(frame)
LineFrame   <- gvisLineChart(frame, xvar=c("xaxis"), yvar="yaxis", options=list(title='Lift Curve for test data CART', legend="right", width=600, height=400, hAxis="{title:'Percent of data', titleTextStyle:{color:'black'}}", vAxes="[{title:' Percent of Class 1'}]",  series="[{color:'green',pointSize:3, targetAxisIndex: 0}]"))
print(LineFrame,'chart')

probs = test_Probability_class1_tree_large
xaxis = sort(unique(c(0,1,probs)), decreasing = TRUE)
res = 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly = which(probs >= 1-prob)
  c(length(useonly)/length(test_actual), sum(test_actual[useonly])/all1s) 
  }))
xaxis = res[1,]; yaxis = res[2,]
names(xaxis)<- NULL; names(yaxis) <- NULL
frame<-cbind(xaxis,yaxis)
frame<-as.data.frame(frame)
LineFrame1   <- gvisLineChart(frame, xvar=c("xaxis"), yvar="yaxis", options=list(title='Lift Curve for test data CART large', legend="right", width=600, height=400, hAxis="{title:'Percent of data', titleTextStyle:{color:'black'}}", vAxes="[{title:'Percent of Class 1'}]",  series="[{color:'green',pointSize:3, targetAxisIndex: 0}]"))
print(LineFrame1,'chart')

probs = test_Probability_class1_log
xaxis = sort(unique(c(0,1,probs)), decreasing = TRUE)
res = 100*Reduce(cbind,lapply(xaxis, function(prob){
  useonly = which(probs >= 1-prob)
  c(length(useonly)/length(test_actual), sum(test_actual[useonly])/all1s) 
  }))
xaxis = res[1,]; yaxis = res[2,]
names(xaxis)<- NULL; names(yaxis) <- NULL
frame<-cbind(xaxis,yaxis)
frame<-as.data.frame(frame)
LineFrame2  <- gvisLineChart(frame, xvar=c("xaxis"), yvar="yaxis", options=list(title='Lift Curve for test data CART large', legend="right", width=600, height=400, hAxis="{title:'Percent of data', titleTextStyle:{color:'black'}}", vAxes="[{title:'Percent of Class 1'}]",  series="[{color:'green',pointSize:3, targetAxisIndex: 0}]"))
print(LineFrame2,'chart')

```
</div>



Profit Matrix
========================================================
<div class="row">
<div class="col-md-6">
```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}

print(xtable(Profit_Matrix ,caption="Assumed Profits and Costs", digits=1,),type="html",html.table.attributes = "class='table table-striped table-hover table-bordered'",caption.placement="top",comment = FALSE,include.rownames = TRUE)

```
</div>
</div>



Profit Curves: Test Data
========================================================

<style>
.wrapper{
            
            
            width: 100%;
           
            overflow-x: scroll;
             
          }
.wrapper1{
            
           height:450px;
             overflow-y: scroll;
          }
</style>
<div class="wrapper wrapper1">

```{r echo=FALSE, warning=FALSE,comment=NA, results='asis',error=FALSE,message=FALSE}
actual_class<- test_data[,dependent_variable]

probs = test_Probability_class1_tree
xaxis = sort(unique(c(0,1,probs)), decreasing = TRUE)
res = Reduce(cbind,lapply(xaxis, function(prob){
  useonly = which(probs >= prob)
  predict_class = 1*(probs >= prob)
  theprofit = Profit_Matrix[1,1]*sum(predict_class==1 & actual_class ==1)+
    Profit_Matrix[1,2]*sum(predict_class==0 & actual_class ==1)+
    Profit_Matrix[2,1]*sum(predict_class==1 & actual_class ==0)+
    Profit_Matrix[2,2]*sum(predict_class==0 & actual_class ==0)
  
  c(100*length(useonly)/length(actual_class), theprofit) 
  }))
xaxis = res[1,]; yaxis = res[2,]
names(xaxis)<- NULL; names(yaxis) <- NULL
frame<-cbind(xaxis,yaxis)
frame<-as.data.frame(frame)
LineFramev1   <- gvisLineChart(frame, xvar=c("xaxis"), yvar="yaxis", options=list(title='Profit Curve for test data CART 1', legend="right", width=600, height=600, hAxis="{title:'Percent Selected', titleTextStyle:{color:'black'}}", vAxes="[{title:'Estimated Profit'}]",  series="[{color:'green',pointSize:3, targetAxisIndex: 0}]"))
print(LineFramev1,'chart')

probs = test_Probability_class1_tree_large
xaxis = sort(unique(c(0,1,probs)), decreasing = TRUE)
res = Reduce(cbind,lapply(xaxis, function(prob){
  useonly = which(probs >= prob)
  predict_class = 1*(probs >= prob)
  theprofit = Profit_Matrix[1,1]*sum(predict_class==1 & actual_class ==1)+
    Profit_Matrix[1,2]*sum(predict_class==0 & actual_class ==1)+
    Profit_Matrix[2,1]*sum(predict_class==1 & actual_class ==0)+
    Profit_Matrix[2,2]*sum(predict_class==0 & actual_class ==0)
  
  c(100*length(useonly)/length(actual_class), theprofit) 
  }))
xaxis = res[1,]; yaxis = res[2,]
names(xaxis)<- NULL; names(yaxis) <- NULL
frame<-cbind(xaxis,yaxis)
frame<-as.data.frame(frame)
LineFramev2   <- gvisLineChart(frame, xvar=c("xaxis"), yvar="yaxis", options=list(title='Profit Curve for test data CART 2', legend="right", width=600, height=400, hAxis="{title:'Percent Selected', titleTextStyle:{color:'black'}}", vAxes="[{title:'Estimated Profit'}]",  series="[{color:'green',pointSize:3, targetAxisIndex: 0}]"))
print(LineFramev2,'chart')

probs = test_Probability_class1_log
xaxis = sort(unique(c(0,1,probs)), decreasing = TRUE)
res = Reduce(cbind,lapply(xaxis, function(prob){
  useonly = which(probs >= prob)
  predict_class = 1*(probs >= prob)
  theprofit = Profit_Matrix[1,1]*sum(predict_class==1 & actual_class ==1)+
    Profit_Matrix[1,2]*sum(predict_class==0 & actual_class ==1)+
    Profit_Matrix[2,1]*sum(predict_class==1 & actual_class ==0)+
    Profit_Matrix[2,2]*sum(predict_class==0 & actual_class ==0)
  
  c(100*length(useonly)/length(actual_class), theprofit) 
  }))
xaxis = res[1,]; yaxis = res[2,]
names(xaxis)<- NULL; names(yaxis) <- NULL
frame<-cbind(xaxis,yaxis)
frame<-as.data.frame(frame)
LineFramev3   <- gvisLineChart(frame, xvar=c("xaxis"), yvar="yaxis", options=list(title='Profit Curve for test data logistic regression', legend="right", width=600, height=400, hAxis="{title:'Percent Selected', titleTextStyle:{color:'black'}}", vAxes="[{title:'Estimated Profit'}]",  series="[{color:'green',pointSize:3, targetAxisIndex: 0}]"))
print(LineFramev3,'chart')
```
</div>



Segment Specific Drivers Analysis
========================================================


What if we do the same analysis but for each segment separately? 

Does it make sense to do so? Why?



Segment Specific Drivers Analysis
========================================================

<style>
.wrapper{
            
            
            width: 100%;
           
            overflow-x: scroll;
             
          }
.wrapper1{
            
           height:400px;
             overflow-y: scroll;
          }
</style>
<div class="wrapper wrapper1">
```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
numb_clusters_used = 5

cluster_file = paste(paste(local_directory,"data", sep="/"),paste(paste(datafile_name,paste("cluster",numb_clusters_used,sep="") , sep="_"), "csv", sep="."), sep="/")

cluster_ids <- read.csv(cluster_file, sep=",", dec=".") # this contains only the matrix ProjectData
cluster_ids <- data.matrix(cluster_ids)
cluster_ids = cluster_ids[,2]

actual_class<- test_data[,dependent_variable]
probs_tree = 0*test_Probability_class1_tree
probs_tree_large = 0*test_Probability_class1_tree_large
probs_log = 0*test_Probability_class1_log
Log_Drivers = NULL

for (i in sort(unique(cluster_ids))){
  useonly = which(cluster_ids==i)
  if (length(useonly) >= min_segment){
    
    test_ids_used = intersect(test_data_ids,useonly)
    probs_to_fill = which(sapply(test_data_ids, function(i) sum(test_ids_used==i)) !=0)
    estimation_data_clus=ProjectData[intersect(estimation_data_ids,useonly) ,]
    test_data_clus=ProjectData[intersect(test_data_ids,useonly),]
    
    ###
    estimation_data_clus_nolabel = cbind(estimation_data_clus[,dependent_variable], estimation_data_clus[,independent_variables])
    colnames(estimation_data_clus_nolabel)<- c(colnames(estimation_data_clus)[dependent_variable],independent_variables_nolabel)
    
    test_data_clus_nolabel = cbind(test_data_clus[,dependent_variable], test_data_clus[,independent_variables])
    colnames(test_data_clus_nolabel)<- c(dependent_variable,independent_variables_nolabel)
    
    estimation_data_clus = data.frame(estimation_data_clus)
    test_data_clus = data.frame(test_data_clus)
    estimation_data_clus_nolabel = data.frame(estimation_data_clus_nolabel)
    test_data_clus_nolabel = data.frame(test_data_clus_nolabel)
    ###
    
    CART_tree<-rpart(formula, data= estimation_data_clus_nolabel,method="class", control=CART_control)    
    CART_tree_large<-rpart(formula, data= estimation_data_clus_nolabel,method="class", control=rpart.control(cp = 0.005))
    logreg_solution <- glm(formula_log, family=binomial(link="logit"), data=estimation_data_clus)
    
    #####
    
    test_Probability_class1_tree<-predict(CART_tree, test_data_clus_nolabel)[,2]
    test_Probability_class1_tree_large<-predict(CART_tree_large, test_data_clus_nolabel)[,2]
    test_Probability_class1_log<-predict(logreg_solution, type="response", newdata=test_data_clus[,independent_variables])
    
    #######
    probs_tree[probs_to_fill] <- test_Probability_class1_tree
    probs_tree_large[probs_to_fill] <- test_Probability_class1_tree
    probs_log[probs_to_fill] <- test_Probability_class1_log
    
    
    log_coefficients = round(summary(logreg_solution)$coefficients,1)
    Log_Drivers_segment = tail(log_coefficients[,"z value", drop=F],-1) # remove the intercept
    Log_Drivers_segment = Log_Drivers_segment/max(abs(Log_Drivers_segment))
    
    tree_importance = CART_tree$variable.importance
    tree_ordered_drivers = as.numeric(gsub("\\IV"," ",names(CART_tree$variable.importance)))
    tree_importance_final = rep(0,length(independent_variables))
    tree_importance_final[tree_ordered_drivers] <- tree_importance
    tree_importance_final <- tree_importance_final/max(abs(tree_importance_final))
    tree_importance_final <- tree_importance_final*sign(Log_Drivers_segment)
    
    #Log_Drivers = cbind(Log_Drivers,tree_importance_final)
    Log_Drivers = cbind(Log_Drivers,Log_Drivers_segment)
    
    
    }
  }
colnames(Log_Drivers) <- paste("Segment", 1:length(unique(cluster_ids)), sep = " ")
#cat(renderHeatmapX(tail(Log_Drivers,-1), border=1, center = 0, minvalue = 0))
print(xtable(tail(Log_Drivers,-1),caption="" , digits=1,),type="html",html.table.attributes = "class='table table-striped table-hover table-bordered'",caption.placement="top",comment = FALSE,include.rownames = TRUE)


```
</div>




Segment Specific Profit Curves: Test Data
========================================================

Expected revenues can increase by 15% in this case: Is that a good number?

<style>
.wrapper{
            
            
            width: 100%;
           
            overflow-x: scroll;
             
          }
.wrapper1{
            
           height:450px;
             overflow-y: scroll;
          }
</style>
<div class="wrapper wrapper1">

```{r echo=FALSE, warning=FALSE,comment=NA, results='asis',error=FALSE,message=FALSE}
actual_class<- test_data[,dependent_variable]

probs = probs_tree
xaxis = sort(unique(c(0,1,probs)), decreasing = TRUE)
res = Reduce(cbind,lapply(xaxis, function(prob){
  useonly = which(probs >= prob)
  predict_class = 1*(probs >= prob)
  theprofit = Profit_Matrix[1,1]*sum(predict_class==1 & actual_class ==1)+
    Profit_Matrix[1,2]*sum(predict_class==0 & actual_class ==1)+
    Profit_Matrix[2,1]*sum(predict_class==1 & actual_class ==0)+
    Profit_Matrix[2,2]*sum(predict_class==0 & actual_class ==0)
  
  c(100*length(useonly)/length(actual_class), theprofit) 
  }))
xaxis = res[1,]; yaxis = res[2,]
names(xaxis)<- NULL; names(yaxis) <- NULL
frame<-cbind(xaxis,yaxis)
frame<-as.data.frame(frame)
LineFramev1   <- gvisLineChart(frame, xvar=c("xaxis"), yvar="yaxis", options=list(title='Profit Curve for test data CART 1', legend="right", width=600, height=600, hAxis="{title:'Percent Selected', titleTextStyle:{color:'black'}}", vAxes="[{title:'Estimated Profit'}]",  series="[{color:'green',pointSize:3, targetAxisIndex: 0}]"))
print(LineFramev1,'chart')

probs = probs_tree_large
xaxis = sort(unique(c(0,1,probs)), decreasing = TRUE)
res = Reduce(cbind,lapply(xaxis, function(prob){
  useonly = which(probs >= prob)
  predict_class = 1*(probs >= prob)
  theprofit = Profit_Matrix[1,1]*sum(predict_class==1 & actual_class ==1)+
    Profit_Matrix[1,2]*sum(predict_class==0 & actual_class ==1)+
    Profit_Matrix[2,1]*sum(predict_class==1 & actual_class ==0)+
    Profit_Matrix[2,2]*sum(predict_class==0 & actual_class ==0)
  
  c(100*length(useonly)/length(actual_class), theprofit) 
  }))
xaxis = res[1,]; yaxis = res[2,]
names(xaxis)<- NULL; names(yaxis) <- NULL
frame<-cbind(xaxis,yaxis)
frame<-as.data.frame(frame)
LineFramev2   <- gvisLineChart(frame, xvar=c("xaxis"), yvar="yaxis", options=list(title='Profit Curve for test data CART 2', legend="right", width=600, height=400, hAxis="{title:'Percent Selected', titleTextStyle:{color:'black'}}", vAxes="[{title:'Estimated Profit'}]",  series="[{color:'green',pointSize:3, targetAxisIndex: 0}]"))
print(LineFramev2,'chart')

probs = probs_log
xaxis = sort(unique(c(0,1,probs)), decreasing = TRUE)
res = Reduce(cbind,lapply(xaxis, function(prob){
  useonly = which(probs >= prob)
  predict_class = 1*(probs >= prob)
  theprofit = Profit_Matrix[1,1]*sum(predict_class==1 & actual_class ==1)+
    Profit_Matrix[1,2]*sum(predict_class==0 & actual_class ==1)+
    Profit_Matrix[2,1]*sum(predict_class==1 & actual_class ==0)+
    Profit_Matrix[2,2]*sum(predict_class==0 & actual_class ==0)
  
  c(100*length(useonly)/length(actual_class), theprofit) 
  }))
xaxis = res[1,]; yaxis = res[2,]
names(xaxis)<- NULL; names(yaxis) <- NULL
frame<-cbind(xaxis,yaxis)
frame<-as.data.frame(frame)
LineFramev3   <- gvisLineChart(frame, xvar=c("xaxis"), yvar="yaxis", options=list(title='Profit Curve for test data logistic regression', legend="right", width=600, height=400, hAxis="{title:'Percent Selected', titleTextStyle:{color:'black'}}", vAxes="[{title:'Estimated Profit'}]",  series="[{color:'green',pointSize:3, targetAxisIndex: 0}]"))
print(LineFramev3,'chart')
```
</div>



Iterative Data Analytics Processes
========================================================

Does segment specific analysis help for our business decisions? 

Which solution should we use? 

Should we explore a different solution? 

Should we re-start from data collection, factor analysis, segmentation, or classification and drivers' analysis? 

How can the company use the final solution?



Observations and Lessons
========================================================

- Fitting data is very easy: prediction is the challenge
- Beware of overfitting: key risk
- Beware of changes in the statistics of the data
- Defining what "success" is can be crucial
- Segment specific analysis must be considered when data are heterogeneous (15% increase in revenues in this case)
- Contextual knowledge is absolutely necessary
- Many performance metrics available
- Deployment of classification models requires a number of managerial decisions
- Iterations are necessary: Efficiency, Replicability, Reusability are key
- Data Analytics requires a balance between quantitative and qualitative criteria: it is "Art AND Science"

 

